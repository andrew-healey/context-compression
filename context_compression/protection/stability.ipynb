{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max difference between fp64 and fp64 fast loop cumsum: tensor(2.3874e-11, device='cuda:0', dtype=torch.float64)\n",
      "Max difference between fp64 and fp32 cumsum: tensor(0.0007, device='cuda:0', dtype=torch.float64)\n",
      "Max difference between fp64 and fp32 sum: tensor(6.2322e-05, device='cuda:0', dtype=torch.float64)\n",
      "Max difference between fp64 and fp64 triton cumsum: tensor(2.3874e-11, device='cuda:0', dtype=torch.float64)\n",
      "Max difference between fp64 and fp64 triton cumsum (converted to fp32): tensor(6.1035e-05, device='cuda:0', dtype=torch.float64)\n",
      "Max diff between fp64 and fp32 triton cumsum: tensor(0.0216, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# ok so we're implementing a custom cumsum function\n",
    "# we want it to be super numerically stable\n",
    "\n",
    "# set seed to 42\n",
    "torch.manual_seed(42)\n",
    "\n",
    "input_data = torch.randn(1000000, dtype=torch.float64, device=\"cuda\")\n",
    "input_data_fp32 = input_data.float()\n",
    "\n",
    "# let's start by checking torch fp64 cumsum. that'll be our ground truth.\n",
    "fp64_cumsum_result = torch.cumsum(input_data, dim=-1)\n",
    "fp64_sum_result = torch.sum(input_data, dim=-1)\n",
    "\n",
    "# let's see how much it diverges from doing a non-recursive for loop. my past experiments suggested this is kinda unstable.\n",
    "# fp64_slow_loop_cumsum = torch.zeros_like(input_data)\n",
    "# for i in range(input_data.shape[-1]):\n",
    "#     fp64_slow_loop_cumsum[..., i] = torch.sum(input_data[..., :i+1], dim=-1)\n",
    "# print(f\"Max difference between fp64 and fp64 loop cumsum:\",(fp64_cumsum - fp64_slow_loop_cumsum).max())\n",
    "# then let's compare to a loop that uses an accumulator.\n",
    "fp64_fast_loop_cumsum = torch.zeros_like(input_data)\n",
    "fp64_fast_loop_cumsum[..., 0] = input_data[..., 0]\n",
    "for i in range(1, input_data.shape[-1]):\n",
    "    fp64_fast_loop_cumsum[..., i] = fp64_fast_loop_cumsum[..., i-1] + input_data[..., i]\n",
    "print(f\"Max difference between fp64 and fp64 fast loop cumsum:\",(fp64_cumsum_result - fp64_fast_loop_cumsum).max())\n",
    "\n",
    "# then we'll compare to torch fp32 cumsum. past experiments say this is super stable and close to fp64 torch.cumsum.\n",
    "fp32_cumsum_result = torch.cumsum(input_data_fp32, dim=-1)\n",
    "print(f\"Max difference between fp64 and fp32 cumsum:\",(fp64_cumsum_result - fp32_cumsum_result).max())\n",
    "\n",
    "fp32_sum_result = torch.sum(input_data_fp32, dim=-1)\n",
    "print(f\"Max difference between fp64 and fp32 sum:\",(fp64_sum_result - fp32_sum_result).max())\n",
    "\n",
    "\n",
    "# then we'll test our own stuff\n",
    "\n",
    "# first a triton kernel that uses fp64. this should be as good as torch fp64 with loop.\n",
    "from stability import triton_cumsum\n",
    "\n",
    "fp64_triton_cumsum_result = triton_cumsum(input_data)\n",
    "print(f\"Max difference between fp64 and fp64 triton cumsum:\",(fp64_cumsum_result - fp64_triton_cumsum_result).max())\n",
    "\n",
    "fp64_triton_cumsum_result_fp32 = fp64_triton_cumsum_result.float()\n",
    "print(f\"Max difference between fp64 and fp64 triton cumsum (converted to fp32):\",(fp64_cumsum_result - fp64_triton_cumsum_result_fp32).max())\n",
    "\n",
    "\n",
    "# then a triton kernel to do parallel scan, I think. we'll do it in fp64 at first. it should hopefully match torch fp64 cumsum super closely.\n",
    "\n",
    "# then we'll do the same thing in fp32.\n",
    "\n",
    "fp32_triton_cumsum_result = triton_cumsum(input_data_fp32)\n",
    "print(\"Max diff between fp64 and fp32 triton cumsum:\",(fp64_cumsum_result - fp32_triton_cumsum_result).max())\n",
    "\n",
    "# then we'll implement our custom protect_and_attack function in fp64.\n",
    "\n",
    "# it should be super close to torch fp64 cumsum too - about as close as our fp64 parallel scan triton kernel.\n",
    "\n",
    "# and as we finalize each of the functions / etc, we'll move it into the stability.py file (for brevity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff between fp64 and bliasson sum: tensor(4.5475e-13, device='cuda:0', dtype=torch.float64)\n",
      "Max diff between fp64 and bliasson sum (converted to fp32): tensor(-0.0002, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# ok let's make smth simple. let's just do a super simple naive algorithm.\n",
    "# so we'll just try to make a rly good sum(inputs) function that takes in a list of inputs and returns a single sum.\n",
    "# it will use a binary tree to accumulate them. it'll fill it out to the nearest power of 2, then treat it like some binary tree (hrrm, let's say w/ narrowing the set of active nodes by just removing the nonzero bits)\n",
    "# ah what's that thing with indexing? should it be one-indexed then? b/c then you can go from the first element (i=1) to its first child (i=2) by multiplying by 2.\n",
    "# yes yes that's it.\n",
    "# hrrm but do we rly *want* to be doubling, or whatever? maybe that's just some random artefact of min-heaps.\n",
    "# nono. I think we want zero-indexed. b/c we're gonna accumulate everything into i=0.\n",
    "\n",
    "import torch\n",
    "\n",
    "def bliasson_sum(x: torch.Tensor):\n",
    "    closest_power_of_2 = 2**(x.shape[-1].bit_length())\n",
    "    x_big = torch.zeros(closest_power_of_2, dtype=x.dtype, device=x.device)\n",
    "    x_big[:x.shape[-1]] = x\n",
    "\n",
    "    # ok so now we're going to do several rounds of summing.\n",
    "    # each round will have half as many active nodes as the previous one. the final round will have 1 active node. after that round, we'll just return the sum.\n",
    "    # ok so we can compute this by just using a diff set of indices each time.\n",
    "\n",
    "    indices = torch.arange(0, closest_power_of_2)\n",
    "\n",
    "    while len(indices) > 1:\n",
    "        assert len(indices) % 2 == 0\n",
    "        next_indices = indices[:len(indices)//2] * 2\n",
    "        x_big[next_indices] = x_big[indices[::2]] + x_big[indices[1::2]]\n",
    "        indices = next_indices\n",
    "    \n",
    "    return x_big[0]\n",
    "\n",
    "assert sum(torch.tensor([1])) == 1\n",
    "assert sum(torch.tensor([1, 2])) == 3\n",
    "assert sum(torch.tensor([1, 2, 3])) == 6\n",
    "assert sum(torch.tensor([1, 2, 3, 4])) == 10\n",
    "assert sum(torch.tensor([1, 2, 3, 4, 5])) == 15\n",
    "assert sum(torch.tensor([1, 2, 3, 4, 5, 6])) == 21\n",
    "assert sum(torch.tensor([1, 2, 3, 4, 5, 6, 7])) == 28\n",
    "\n",
    "def bliasson_cumsum(x: torch.Tensor):\n",
    "    return torch.tensor([bliasson_sum(x[:i+1]) for i in range(x.shape[-1])])\n",
    "\n",
    "assert (bliasson_cumsum(torch.tensor([1, 2, 3, 4, 5, 6, 7])) == torch.tensor([1, 3, 6, 10, 15, 21, 28])).all()\n",
    "\n",
    "\n",
    "\n",
    "# now let's check the diff versus torch fp64 cumsum.\n",
    "\n",
    "bliasson_sum_result_fp64 = bliasson_sum(input_data)\n",
    "print(f\"Max diff between fp64 and bliasson sum:\",(fp64_sum_result - bliasson_sum_result_fp64).max())\n",
    "\n",
    "# now let's try it in fp32.\n",
    "\n",
    "bliasson_sum_result_fp32 = bliasson_sum(input_data_fp32)\n",
    "print(f\"Max diff between fp64 and bliasson sum (converted to fp32):\",(fp64_sum_result - bliasson_sum_result_fp32).max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff between fp64 and bliasson cumsum: tensor(1.3642e-12, device='cuda:0', dtype=torch.float64)\n",
      "Max diff between fp64 and bliasson cumsum on fp32: tensor(0.0005, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from typing import Callable\n",
    "\n",
    "def bliasson_associative_scan(x: torch.Tensor, merge_fn: Callable=lambda l, r: l + r, dim: int=-1):\n",
    "    x = x.transpose(dim, x.ndim-1) if dim != -1 else x\n",
    "    *rest, n = x.shape\n",
    "    bit_length = n.bit_length()\n",
    "    closest_power_of_2 = 2**bit_length\n",
    "    x_big = torch.zeros((*rest, closest_power_of_2), dtype=x.dtype, device=x.device)\n",
    "    x_big[...,:n] = x\n",
    "\n",
    "    # ok so now we're going to do several rounds of summing.\n",
    "    # each round will have half as many active nodes as the previous one. the final round will have 1 active node. after that round, we'll just return the sum.\n",
    "    # ok so we can compute this by just using a diff set of indices each time.\n",
    "\n",
    "    indices = torch.arange(0, closest_power_of_2)\n",
    "\n",
    "    while len(indices) > 1:\n",
    "        assert len(indices) % 2 == 0\n",
    "        next_indices = indices[1::2]\n",
    "        x_big[...,next_indices] = merge_fn(x_big[...,indices[::2]], x_big[...,indices[1::2]])\n",
    "        indices = next_indices\n",
    "    \n",
    "    # ok now we're going to propagate the info back down the tree, from top-down.\n",
    "\n",
    "    for i in range(bit_length,1,-1):\n",
    "        end_of_first_chunk = torch.arange(2 ** (i-1),closest_power_of_2,2 ** (i-1)) - 1\n",
    "        end_of_first_half_of_second_chunk = end_of_first_chunk + 2 ** (i - 2)\n",
    "\n",
    "        x_big[...,end_of_first_half_of_second_chunk] = merge_fn(x_big[...,end_of_first_chunk], x_big[...,end_of_first_half_of_second_chunk])\n",
    "    \n",
    "    raw_out = x_big[...,:n]\n",
    "\n",
    "    return raw_out.transpose(dim, x.ndim-1) if dim != -1 else raw_out\n",
    "\n",
    "(bliasson_associative_scan(torch.tensor([1, 2, 3, 4, 5, 6, 7])) == torch.tensor([1, 3, 6, 10, 15, 21, 28])).all()\n",
    "\n",
    "bliasson_cumsum_result_fp64 = bliasson_associative_scan(input_data)\n",
    "print(f\"Max diff between fp64 and bliasson cumsum:\",(fp64_cumsum_result - bliasson_cumsum_result_fp64).max())\n",
    "\n",
    "bliasson_cumsum_result_fp32 = bliasson_associative_scan(input_data_fp32)\n",
    "print(f\"Max diff between fp64 and bliasson cumsum on fp32:\",(fp64_cumsum_result - bliasson_cumsum_result_fp32).max())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_51432/580013728.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  inputs_causing_instability = torch.load(\"../../inputs_causing_instability.pt\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff during training: 1.71661376953125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.4332275390625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.52587890625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.0517578125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.239776611328125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 7.62939453125e-06 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.33514404296875e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 5.960464477539063e-08 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.71661376953125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.52587890625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.5497207641601562e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.049041748046875e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.7642974853515625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.33514404296875e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.52587890625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.1444091796875e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.52587890625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.71661376953125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.9073486328125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.33514404296875e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.1920928955078125e-07 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.71661376953125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 4.76837158203125e-07 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 9.5367431640625e-07 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.9073486328125e-06 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 2.288818359375e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 2.288818359375e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.0517578125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.814697265625e-06 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.9073486328125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.9073486328125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.0517578125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.0517578125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.337860107421875e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 4.1961669921875e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.0517578125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.814697265625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.52587890625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.52587890625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 2.288818359375e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.0517578125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.7670135498046875e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 4.1961669921875e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 2.288818359375e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 6.103515625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 6.008148193359375e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 4.57763671875e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0001220703125 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 4.57763671875e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 4.57763671875e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0001068115234375 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.814697265625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 4.57763671875e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 6.103515625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 6.103515625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 7.62939453125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 6.103515625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 6.866455078125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 4.57763671875e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 6.103515625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 6.103515625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 6.103515625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.814697265625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.4332275390625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 6.103515625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 9.918212890625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 5.340576171875e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 7.104873657226562e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.0517578125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 6.4849853515625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.814697265625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 7.62939453125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 9.1552734375e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 6.103515625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0001220703125 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 7.62939453125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 6.866455078125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 7.62939453125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 7.62939453125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 6.103515625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0001068115234375 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 6.866455078125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 6.103515625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 6.866455078125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 9.1552734375e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 6.103515625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 4.9591064453125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 6.103515625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 6.103515625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 6.103515625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 6.103515625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 6.103515625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 7.62939453125e-05 | Max diff with bliasson cumsum: 0.0\n"
     ]
    }
   ],
   "source": [
    "# now let's test our bliasson cumsum on our dataset of inputs that cause instability.\n",
    "\n",
    "inputs_causing_instability = torch.load(\"../../inputs_causing_instability.pt\")\n",
    "\n",
    "for S_64_np, max_diff in inputs_causing_instability:\n",
    "    S_64 = torch.from_numpy(S_64_np)\n",
    "    FF_64 = torch.cumsum(S_64, dim=-2)\n",
    "    bliasson_cumsum_result_fp64 = bliasson_associative_scan(S_64,dim=1)\n",
    "    print(f\"Max diff during training: {max_diff} | Max diff with bliasson cumsum: {(FF_64 - bliasson_cumsum_result_fp64).max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protect-and-attack algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ok so we have a slightly diff merge function.\n",
    "# acc, for now, let's use unpack and pack.\n",
    "# then we can just drop in a fully custom merge function into our bliasson algorithm.\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "from typing import Tuple\n",
    "\n",
    "# Manual tuple packing by @jackd from https://github.com/openai/triton/issues/2359\n",
    "@triton.jit\n",
    "def unpack64(merged):\n",
    "    tl.static_assert(merged.dtype == tl.uint64)\n",
    "    b = (merged & 0xFFFFFFFF).to(tl.uint32).to(tl.float32, bitcast=True)\n",
    "    a = (merged >> 32).to(tl.uint32).to(tl.float32, bitcast=True)\n",
    "    return a, b\n",
    "\n",
    "@triton.jit\n",
    "def pack64(a, b):\n",
    "    tl.static_assert(a.dtype == tl.float32)\n",
    "    tl.static_assert(b.dtype == tl.float32)\n",
    "    a = a.to(dtype=tl.uint32, bitcast=True).to(tl.uint64)\n",
    "    a = a << 32\n",
    "    b = b.to(dtype=tl.uint32, bitcast=True).to(tl.uint64)\n",
    "    return a | b\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# These are the new Triton kernels that wrap the util functions.\n",
    "# They operate elementwise over flattened arrays.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "@triton.jit\n",
    "def pack_kernel(a_ptr, b_ptr, out_ptr, numel, BLOCK_SIZE: tl.constexpr):\n",
    "    \"\"\"\n",
    "    Triton kernel for packing two FP32 arrays into one FP64 output.\n",
    "    \n",
    "    a_ptr : pointer to the first FP32 input tensor.\n",
    "    b_ptr : pointer to the second FP32 input tensor.\n",
    "    out_ptr : pointer to the output FP64 tensor. (will hold the bit-packed result)\n",
    "    numel : number of elements to process\n",
    "    BLOCK_SIZE : compile-time constant block size.\n",
    "    \"\"\"\n",
    "    pid = tl.program_id(0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < numel\n",
    "\n",
    "    # Load FP32 values from the two inputs.\n",
    "    a = tl.load(a_ptr + offsets, mask=mask)\n",
    "    b = tl.load(b_ptr + offsets, mask=mask)\n",
    "\n",
    "    # Pack the two FP32 values into a single uint64.\n",
    "    merged = pack64(a, b)\n",
    "    # Bitcast our 64-bit unsigned integer into a FP64.\n",
    "    merged_fp64 = merged.to(tl.float64, bitcast=True)\n",
    "    tl.store(out_ptr + offsets, merged_fp64, mask=mask)\n",
    "\n",
    "@triton.jit\n",
    "def unpack_kernel(in_ptr, a_ptr, b_ptr, numel, BLOCK_SIZE: tl.constexpr):\n",
    "    \"\"\"\n",
    "    Triton kernel for unpacking an FP64 array into two FP32 outputs.\n",
    "    \n",
    "    in_ptr : pointer to the input FP64 tensor (packed data)\n",
    "    a_ptr : pointer to the first FP32 output tensor.\n",
    "    b_ptr : pointer to the second FP32 output tensor.\n",
    "    numel : number of elements to process\n",
    "    BLOCK_SIZE : compile-time constant block size.\n",
    "    \"\"\"\n",
    "    pid = tl.program_id(0)\n",
    "    block_start = pid * BLOCK_SIZE\n",
    "    offsets = block_start + tl.arange(0, BLOCK_SIZE)\n",
    "    mask = offsets < numel\n",
    "\n",
    "    # Load the packed FP64 data then bitcast it to uint64.\n",
    "    merged_fp64 = tl.load(in_ptr + offsets, mask=mask)\n",
    "    merged = merged_fp64.to(tl.uint64, bitcast=True)\n",
    "    a, b = unpack64(merged)\n",
    "    tl.store(a_ptr + offsets, a, mask=mask)\n",
    "    tl.store(b_ptr + offsets, b, mask=mask)\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# PyTorch wrappers for the new Triton kernels.\n",
    "# These functions ensure the inputs are properly formatted, then launch the kernels.\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "def pack_tensors(a: torch.Tensor, b: torch.Tensor, out: torch.Tensor = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Packs two FP32 tensors elementwise into one FP64 tensor using a Triton kernel.\n",
    "    \n",
    "    Args:\n",
    "        a (torch.Tensor): Input tensor of type torch.float32.\n",
    "        b (torch.Tensor): Input tensor of type torch.float32 (must have same shape as a).\n",
    "        out (torch.Tensor, optional): Output tensor to store the result.\n",
    "            If None, a new tensor is allocated.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Packed tensor of type torch.float64.\n",
    "    \"\"\"\n",
    "    if a.shape != b.shape:\n",
    "        raise ValueError(\"Input tensors 'a' and 'b' must have the same shape.\")\n",
    "\n",
    "    numel = a.numel()\n",
    "    device = a.device\n",
    "\n",
    "    if out is None:\n",
    "        out = torch.empty_like(a, dtype=torch.float64, device=device)\n",
    "    else:\n",
    "        if out.shape != a.shape:\n",
    "            raise ValueError(\"Output tensor must have the same shape as input tensors.\")\n",
    "        if out.dtype != torch.float64:\n",
    "            raise ValueError(\"Output tensor must be of type torch.float64.\")\n",
    "\n",
    "    # Ensure the tensors are contiguous.\n",
    "    a = a.contiguous()\n",
    "    b = b.contiguous()\n",
    "    out = out.contiguous()\n",
    "\n",
    "    # Define the grid for launching the kernel.\n",
    "    grid = lambda meta: (triton.cdiv(numel, meta[\"BLOCK_SIZE\"]),)\n",
    "    pack_kernel[grid](a, b, out, numel, BLOCK_SIZE=1024)\n",
    "    return out\n",
    "\n",
    "def unpack_tensor(merged: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Unpacks an FP64 (packed) tensor into two FP32 tensors using a Triton kernel.\n",
    "    \n",
    "    Args:\n",
    "        merged (torch.Tensor): Packed input tensor of type torch.float64.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: Two unpacked tensors of type torch.float32.\n",
    "    \"\"\"\n",
    "    if merged.dtype != torch.float64:\n",
    "        raise ValueError(\"Input tensor must be of type torch.float64.\")\n",
    "\n",
    "    numel = merged.numel()\n",
    "    device = merged.device\n",
    "\n",
    "    a = torch.empty_like(merged, dtype=torch.float32, device=device)\n",
    "    b = torch.empty_like(merged, dtype=torch.float32, device=device)\n",
    "\n",
    "    # Ensure the tensors are contiguous.\n",
    "    merged = merged.contiguous()\n",
    "    a = a.contiguous()\n",
    "    b = b.contiguous()\n",
    "\n",
    "    # Define the grid for launching the kernel.\n",
    "    grid = lambda meta: (triton.cdiv(numel, meta[\"BLOCK_SIZE\"]),)\n",
    "    unpack_kernel[grid](merged, a, b, numel, BLOCK_SIZE=1024)\n",
    "    return a, b\n",
    "\n",
    "\n",
    "# tests\n",
    "\n",
    "a,b = torch.randn(1000000,device=\"cuda\"), torch.randn(1000000,device=\"cuda\")\n",
    "\n",
    "c = pack_tensors(a,b)\n",
    "\n",
    "d,e = unpack_tensor(c)\n",
    "\n",
    "assert (a == d).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input  tensor([0., 0., 0.], device='cuda:0') tensor([1., 2., 3.], device='cuda:0')\n",
      "output tensor([0., 0., 0.], device='cuda:0') tensor([1., 3., 6.], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# ok so let's define merging\n",
    "\n",
    "def merge_fn(C1,C2):\n",
    "    # unpack l into U and V\n",
    "    U1,V1 = unpack_tensor(C1)\n",
    "    # unpack r into W and X\n",
    "    U2,V2 = unpack_tensor(C2)\n",
    "\n",
    "    V = V1 + V2 - torch.minimum(U1,V2)\n",
    "    U = U2 + torch.relu(U1-V2)\n",
    "\n",
    "    return pack_tensors(U,V)\n",
    "\n",
    "def protect_and_attack(A,P,dim=-1):\n",
    "    A_hat = (A - P).float()\n",
    "    U = torch.relu(-A_hat)\n",
    "    V = torch.relu(A_hat)\n",
    "\n",
    "    C = pack_tensors(U,V)\n",
    "\n",
    "    C_out = bliasson_associative_scan(C,merge_fn,dim)\n",
    "\n",
    "    U_out,V_out = unpack_tensor(C_out)\n",
    "\n",
    "    return -1*V_out\n",
    "\n",
    "# ok let's construct some inputs\n",
    "# so for A>=0, P=0, it should match cumsum exactly.\n",
    "\n",
    "A = torch.tensor([1,2,3],device=\"cuda\")\n",
    "P = torch.zeros_like(A,device=\"cuda\")\n",
    "\n",
    "torch.testing.assert_close(protect_and_attack(A,P), torch.tensor([-1.,-3.,-6.],device=\"cuda\"))\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
