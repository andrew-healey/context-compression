{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max difference between fp64 and fp64 fast loop cumsum: tensor(267.5462, device='cuda:0', dtype=torch.float64)\n",
      "Max difference between fp64 and fp32 cumsum: tensor(0.0007, device='cuda:0', dtype=torch.float64)\n",
      "Max difference between fp64 and fp64 triton cumsum: tensor(2.3874e-11, device='cuda:0', dtype=torch.float64)\n",
      "Max difference between fp64 and fp64 triton cumsum (converted to fp32): tensor(6.1035e-05, device='cuda:0', dtype=torch.float64)\n",
      "Max diff between fp64 and fp32 triton cumsum: tensor(0.0216, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# ok so we're implementing a custom cumsum function\n",
    "# we want it to be super numerically stable\n",
    "\n",
    "# set seed to 42\n",
    "torch.manual_seed(42)\n",
    "\n",
    "input_data = torch.randn(1000000, dtype=torch.float64, device=\"cuda\")\n",
    "input_data_fp32 = input_data.float()\n",
    "\n",
    "# let's start by checking torch fp64 cumsum. that'll be our ground truth.\n",
    "fp64_cumsum = torch.cumsum(input_data, dim=-1)\n",
    "# let's see how much it diverges from doing a non-recursive for loop. my past experiments suggested this is kinda unstable.\n",
    "# fp64_slow_loop_cumsum = torch.zeros_like(input_data)\n",
    "# for i in range(input_data.shape[-1]):\n",
    "#     fp64_slow_loop_cumsum[..., i] = torch.sum(input_data[..., :i+1], dim=-1)\n",
    "# print(f\"Max difference between fp64 and fp64 loop cumsum:\",(fp64_cumsum - fp64_slow_loop_cumsum).max())\n",
    "# then let's compare to a loop that uses an accumulator.\n",
    "# fp64_fast_loop_cumsum = torch.zeros_like(input_data)\n",
    "# fp64_fast_loop_cumsum[..., 0] = input_data[..., 0]\n",
    "# for i in range(1, input_data.shape[-1]):\n",
    "#     fp64_fast_loop_cumsum[..., i] = fp64_fast_loop_cumsum[..., i-1] + input_data[..., i]\n",
    "print(f\"Max difference between fp64 and fp64 fast loop cumsum:\",(fp64_cumsum - fp64_fast_loop_cumsum).max())\n",
    "\n",
    "# then we'll compare to torch fp32 cumsum. past experiments say this is super stable and close to fp64 torch.cumsum.\n",
    "fp32_cumsum = torch.cumsum(input_data_fp32, dim=-1)\n",
    "print(f\"Max difference between fp64 and fp32 cumsum:\",(fp64_cumsum - fp32_cumsum).max())\n",
    "\n",
    "# then we'll test our own stuff\n",
    "\n",
    "# first a triton kernel that uses fp64. this should be as good as torch fp64 with loop.\n",
    "\n",
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def cumsum_triton(x_ptr, y_ptr, n_elements):\n",
    "    \"\"\"\n",
    "    A simple sequential cumulative sum kernel in fp64.\n",
    "    \n",
    "    This kernel reads the input array from x_ptr, initializes the accumulator\n",
    "    with the first element (to ensure the accumulator is fp64) and writes the\n",
    "    cumulative sum into y_ptr. It then loops over the remaining elements,\n",
    "    adds each value to the accumulator, and stores the cumulative sum.\n",
    "    \n",
    "    Parameters:\n",
    "      x_ptr: pointer to the beginning of the input array.\n",
    "      y_ptr: pointer to the beginning of the output array.\n",
    "      n_elements: total number of elements to process (expected to be a Python int).\n",
    "    \"\"\"\n",
    "    # Initialize the accumulator with the first element, ensuring the type is fp64.\n",
    "    acc = tl.load(x_ptr)\n",
    "    tl.store(y_ptr, acc)\n",
    "    \n",
    "    # Process the rest of the elements.\n",
    "    i = 1\n",
    "    while i < n_elements:\n",
    "        val = tl.load(x_ptr + i)\n",
    "        acc = acc + val\n",
    "        tl.store(y_ptr + i, acc)\n",
    "        i = i + 1\n",
    "\n",
    "def fp64_triton_cumsum(x):\n",
    "    \"\"\"\n",
    "    Computes the cumulative sum of the input tensor x (in fp64) using a Triton kernel.\n",
    "    \n",
    "    The function allocates an output tensor of the same shape as x and then launches\n",
    "    the Triton kernel `cumsum_triton` with a grid size of 1 (i.e. a single program instance)\n",
    "    to perform the sequential accumulation.\n",
    "    \n",
    "    Parameters:\n",
    "      x: a 1-dimensional torch tensor of dtype torch.float64 on the CUDA device.\n",
    "    \n",
    "    Returns:\n",
    "      A torch tensor with the cumulative sum computed elementwise.\n",
    "    \"\"\"\n",
    "    n_elements = x.numel()       # total number of elements in x\n",
    "    y = torch.empty_like(x)      # allocate output tensor\n",
    "    # Launch a single kernel instance; note: this kernel assumes n_elements >= 1.\n",
    "    grid = (1,)\n",
    "    cumsum_triton[grid](x, y, n_elements)\n",
    "    return y\n",
    "\n",
    "fp64_triton_cumsum_result = fp64_triton_cumsum(input_data)\n",
    "print(f\"Max difference between fp64 and fp64 triton cumsum:\",(fp64_cumsum - fp64_triton_cumsum_result).max())\n",
    "\n",
    "fp64_triton_cumsum_result_fp32 = fp64_triton_cumsum_result.float()\n",
    "print(f\"Max difference between fp64 and fp64 triton cumsum (converted to fp32):\",(fp64_cumsum - fp64_triton_cumsum_result_fp32).max())\n",
    "\n",
    "\n",
    "# then a triton kernel to do parallel scan, I think. we'll do it in fp64 at first. it should hopefully match torch fp64 cumsum super closely.\n",
    "\n",
    "# then we'll do the same thing in fp32.\n",
    "\n",
    "fp32_triton_cumsum_result = fp64_triton_cumsum(input_data_fp32)\n",
    "print(\"Max diff between fp64 and fp32 triton cumsum:\",(fp64_cumsum - fp32_triton_cumsum_result).max())\n",
    "\n",
    "# then we'll implement our custom protect_and_attack function in fp64.\n",
    "\n",
    "# it should be super close to torch fp64 cumsum too - about as close as our fp64 parallel scan triton kernel.\n",
    "\n",
    "# and as we finalize each of the functions / etc, we'll move it into the stability.py file (for brevity)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
