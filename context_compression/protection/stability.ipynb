{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max difference between fp64 and fp64 fast loop cumsum: tensor(2.3874e-11, device='cuda:0', dtype=torch.float64)\n",
      "Max difference between fp64 and fp32 cumsum: tensor(0.0007, device='cuda:0', dtype=torch.float64)\n",
      "Max difference between fp64 and fp32 sum: tensor(6.2322e-05, device='cuda:0', dtype=torch.float64)\n",
      "Max difference between fp64 and fp64 triton cumsum: tensor(2.3874e-11, device='cuda:0', dtype=torch.float64)\n",
      "Max difference between fp64 and fp64 triton cumsum (converted to fp32): tensor(6.1035e-05, device='cuda:0', dtype=torch.float64)\n",
      "Max diff between fp64 and fp32 triton cumsum: tensor(0.0216, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# ok so we're implementing a custom cumsum function\n",
    "# we want it to be super numerically stable\n",
    "\n",
    "# set seed to 42\n",
    "torch.manual_seed(42)\n",
    "\n",
    "input_data = torch.randn(1000000, dtype=torch.float64, device=\"cuda\")\n",
    "input_data_fp32 = input_data.float()\n",
    "\n",
    "# let's start by checking torch fp64 cumsum. that'll be our ground truth.\n",
    "fp64_cumsum_result = torch.cumsum(input_data, dim=-1)\n",
    "fp64_sum_result = torch.sum(input_data, dim=-1)\n",
    "\n",
    "# let's see how much it diverges from doing a non-recursive for loop. my past experiments suggested this is kinda unstable.\n",
    "# fp64_slow_loop_cumsum = torch.zeros_like(input_data)\n",
    "# for i in range(input_data.shape[-1]):\n",
    "#     fp64_slow_loop_cumsum[..., i] = torch.sum(input_data[..., :i+1], dim=-1)\n",
    "# print(f\"Max difference between fp64 and fp64 loop cumsum:\",(fp64_cumsum - fp64_slow_loop_cumsum).max())\n",
    "# then let's compare to a loop that uses an accumulator.\n",
    "fp64_fast_loop_cumsum = torch.zeros_like(input_data)\n",
    "fp64_fast_loop_cumsum[..., 0] = input_data[..., 0]\n",
    "for i in range(1, input_data.shape[-1]):\n",
    "    fp64_fast_loop_cumsum[..., i] = fp64_fast_loop_cumsum[..., i-1] + input_data[..., i]\n",
    "print(f\"Max difference between fp64 and fp64 fast loop cumsum:\",(fp64_cumsum_result - fp64_fast_loop_cumsum).max())\n",
    "\n",
    "# then we'll compare to torch fp32 cumsum. past experiments say this is super stable and close to fp64 torch.cumsum.\n",
    "fp32_cumsum_result = torch.cumsum(input_data_fp32, dim=-1)\n",
    "print(f\"Max difference between fp64 and fp32 cumsum:\",(fp64_cumsum_result - fp32_cumsum_result).max())\n",
    "\n",
    "fp32_sum_result = torch.sum(input_data_fp32, dim=-1)\n",
    "print(f\"Max difference between fp64 and fp32 sum:\",(fp64_sum_result - fp32_sum_result).max())\n",
    "\n",
    "\n",
    "# then we'll test our own stuff\n",
    "\n",
    "# first a triton kernel that uses fp64. this should be as good as torch fp64 with loop.\n",
    "from stability import triton_cumsum\n",
    "\n",
    "fp64_triton_cumsum_result = triton_cumsum(input_data)\n",
    "print(f\"Max difference between fp64 and fp64 triton cumsum:\",(fp64_cumsum_result - fp64_triton_cumsum_result).max())\n",
    "\n",
    "fp64_triton_cumsum_result_fp32 = fp64_triton_cumsum_result.float()\n",
    "print(f\"Max difference between fp64 and fp64 triton cumsum (converted to fp32):\",(fp64_cumsum_result - fp64_triton_cumsum_result_fp32).max())\n",
    "\n",
    "\n",
    "# then a triton kernel to do parallel scan, I think. we'll do it in fp64 at first. it should hopefully match torch fp64 cumsum super closely.\n",
    "\n",
    "# then we'll do the same thing in fp32.\n",
    "\n",
    "fp32_triton_cumsum_result = triton_cumsum(input_data_fp32)\n",
    "print(\"Max diff between fp64 and fp32 triton cumsum:\",(fp64_cumsum_result - fp32_triton_cumsum_result).max())\n",
    "\n",
    "# then we'll implement our custom protect_and_attack function in fp64.\n",
    "\n",
    "# it should be super close to torch fp64 cumsum too - about as close as our fp64 parallel scan triton kernel.\n",
    "\n",
    "# and as we finalize each of the functions / etc, we'll move it into the stability.py file (for brevity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff between fp64 and bliasson sum: tensor(4.5475e-13, device='cuda:0', dtype=torch.float64)\n",
      "Max diff between fp64 and bliasson sum (converted to fp32): tensor(-0.0002, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# ok let's make smth simple. let's just do a super simple naive algorithm.\n",
    "# so we'll just try to make a rly good sum(inputs) function that takes in a list of inputs and returns a single sum.\n",
    "# it will use a binary tree to accumulate them. it'll fill it out to the nearest power of 2, then treat it like some binary tree (hrrm, let's say w/ narrowing the set of active nodes by just removing the nonzero bits)\n",
    "# ah what's that thing with indexing? should it be one-indexed then? b/c then you can go from the first element (i=1) to its first child (i=2) by multiplying by 2.\n",
    "# yes yes that's it.\n",
    "# hrrm but do we rly *want* to be doubling, or whatever? maybe that's just some random artefact of min-heaps.\n",
    "# nono. I think we want zero-indexed. b/c we're gonna accumulate everything into i=0.\n",
    "\n",
    "import torch\n",
    "\n",
    "def bliasson_sum(x: torch.Tensor):\n",
    "    closest_power_of_2 = 2**(x.shape[-1].bit_length())\n",
    "    x_big = torch.zeros(closest_power_of_2, dtype=x.dtype, device=x.device)\n",
    "    x_big[:x.shape[-1]] = x\n",
    "\n",
    "    # ok so now we're going to do several rounds of summing.\n",
    "    # each round will have half as many active nodes as the previous one. the final round will have 1 active node. after that round, we'll just return the sum.\n",
    "    # ok so we can compute this by just using a diff set of indices each time.\n",
    "\n",
    "    indices = torch.arange(0, closest_power_of_2)\n",
    "\n",
    "    while len(indices) > 1:\n",
    "        assert len(indices) % 2 == 0\n",
    "        next_indices = indices[:len(indices)//2] * 2\n",
    "        x_big[next_indices] = x_big[indices[::2]] + x_big[indices[1::2]]\n",
    "        indices = next_indices\n",
    "    \n",
    "    return x_big[0]\n",
    "\n",
    "assert sum(torch.tensor([1])) == 1\n",
    "assert sum(torch.tensor([1, 2])) == 3\n",
    "assert sum(torch.tensor([1, 2, 3])) == 6\n",
    "assert sum(torch.tensor([1, 2, 3, 4])) == 10\n",
    "assert sum(torch.tensor([1, 2, 3, 4, 5])) == 15\n",
    "assert sum(torch.tensor([1, 2, 3, 4, 5, 6])) == 21\n",
    "assert sum(torch.tensor([1, 2, 3, 4, 5, 6, 7])) == 28\n",
    "\n",
    "def bliasson_cumsum(x: torch.Tensor):\n",
    "    return torch.tensor([bliasson_sum(x[:i+1]) for i in range(x.shape[-1])])\n",
    "\n",
    "assert (bliasson_cumsum(torch.tensor([1, 2, 3, 4, 5, 6, 7])) == torch.tensor([1, 3, 6, 10, 15, 21, 28])).all()\n",
    "\n",
    "\n",
    "\n",
    "# now let's check the diff versus torch fp64 cumsum.\n",
    "\n",
    "bliasson_sum_result_fp64 = bliasson_sum(input_data)\n",
    "print(f\"Max diff between fp64 and bliasson sum:\",(fp64_sum_result - bliasson_sum_result_fp64).max())\n",
    "\n",
    "# now let's try it in fp32.\n",
    "\n",
    "bliasson_sum_result_fp32 = bliasson_sum(input_data_fp32)\n",
    "print(f\"Max diff between fp64 and bliasson sum (converted to fp32):\",(fp64_sum_result - bliasson_sum_result_fp32).max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff between fp64 and bliasson cumsum: tensor(1.8190e-12, device='cuda:0', dtype=torch.float64)\n",
      "Max diff between fp64 and bliasson cumsum on fp32: tensor(0.0005, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def bliasson_cumsum(x: torch.Tensor, dim: int=-1):\n",
    "    x = x.transpose(dim, x.ndim-1) if dim != -1 else x\n",
    "    *rest, n = x.shape\n",
    "    bit_length = n.bit_length()\n",
    "    closest_power_of_2 = 2**bit_length\n",
    "    x_big = torch.zeros((*rest, closest_power_of_2), dtype=x.dtype, device=x.device)\n",
    "    x_big[...,:n] = x\n",
    "\n",
    "    # ok so now we're going to do several rounds of summing.\n",
    "    # each round will have half as many active nodes as the previous one. the final round will have 1 active node. after that round, we'll just return the sum.\n",
    "    # ok so we can compute this by just using a diff set of indices each time.\n",
    "\n",
    "    indices = torch.arange(0, closest_power_of_2)\n",
    "\n",
    "    while len(indices) > 1:\n",
    "        assert len(indices) % 2 == 0\n",
    "        next_indices = indices[1::2]\n",
    "        x_big[...,next_indices] = x_big[...,indices[::2]] + x_big[...,indices[1::2]]\n",
    "        indices = next_indices\n",
    "    \n",
    "    # ok now we're going to propagate the info back down the tree, from top-down.\n",
    "\n",
    "    for i in range(bit_length,1,-1):\n",
    "        end_of_first_chunk = torch.arange(2 ** (i-1),closest_power_of_2,2 ** (i-1)) - 1\n",
    "        end_of_first_half_of_second_chunk = end_of_first_chunk + 2 ** (i - 2)\n",
    "\n",
    "        x_big[...,end_of_first_half_of_second_chunk] += x_big[...,end_of_first_chunk]\n",
    "    \n",
    "    raw_out = x_big[...,:n]\n",
    "\n",
    "    return raw_out.transpose(dim, x.ndim-1) if dim != -1 else raw_out\n",
    "\n",
    "(bliasson_cumsum(torch.tensor([1, 2, 3, 4, 5, 6, 7])) == torch.tensor([1, 3, 6, 10, 15, 21, 28])).all()\n",
    "\n",
    "bliasson_cumsum_result_fp64 = bliasson_cumsum(input_data)\n",
    "print(f\"Max diff between fp64 and bliasson cumsum:\",(fp64_cumsum_result - bliasson_cumsum_result_fp64).max())\n",
    "\n",
    "bliasson_cumsum_result_fp32 = bliasson_cumsum(input_data_fp32)\n",
    "print(f\"Max diff between fp64 and bliasson cumsum on fp32:\",(fp64_cumsum_result - bliasson_cumsum_result_fp32).max())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max diff during training: 2.288818359375e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.1444091796875e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.52587890625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.52587890625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.52587890625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.537799835205078e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.52587890625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 5.364418029785156e-07 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.52587890625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.430511474609375e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.5735626220703125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.1444091796875e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.9073486328125e-06 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.1920928955078125e-07 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.621246337890625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.52587890625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.52587890625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.811981201171875e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.71661376953125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 2.5391578674316406e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 2.1576881408691406e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.814697265625e-06 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.0517578125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.000152587890625 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.0517578125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.000152587890625 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 6.103515625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 4.410743713378906e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 4.9591064453125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 4.57763671875e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.4332275390625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.000244140625 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.000244140625 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.00018310546875 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.000152587890625 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 2.6702880859375e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 2.288818359375e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.814697265625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 1.52587890625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 4.57763671875e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.0517578125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.0517578125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.0517578125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 2.288818359375e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 2.6702880859375e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.62396240234375e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.0517578125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.4332275390625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.814697265625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.24249267578125e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.910064697265625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 3.814697265625e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.03125 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 4.57763671875e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 4.57763671875e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 8.392333984375e-05 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.001220703125 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.25 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.021484375 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n",
      "Max diff during training: 0.0 | Max diff with bliasson cumsum: 0.0\n"
     ]
    }
   ],
   "source": [
    "# now let's test our bliasson cumsum on our dataset of inputs that cause instability.\n",
    "\n",
    "# inputs_causing_instability = torch.load(\"../../inputs_causing_instability.pt\")\n",
    "\n",
    "for S_64_np, max_diff in inputs_causing_instability:\n",
    "    S_64 = torch.from_numpy(S_64_np)\n",
    "    FF_64 = torch.cumsum(S_64, dim=-2)\n",
    "    bliasson_cumsum_result_fp64 = bliasson_cumsum(S_64,dim=1)\n",
    "    print(f\"Max diff during training: {max_diff} | Max diff with bliasson cumsum: {(FF_64 - bliasson_cumsum_result_fp64).max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Protect-and-attack algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
